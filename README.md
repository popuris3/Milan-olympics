




# Milan Olympics Website

This is a sample project to practice Git and GitHub workflow.

## What I Learned
- Git basics (init, add, commit, push)
- Connecting local repo to GitHub
- Branch management
- Preparing project for CI/CD pipelines

## Next Steps
- Add GitHub Actions pipeline
- Connect to Azure DevOps pipeline
- Add deployment automation

## About Me
I work on Azure DevOps migrations, Copado integrations, and automation using Power Automate.
Building GitHub projects to improve DevOps skills and Cloud Architect path.
dataset at `data/latest_medals_source.csv`, and the pipeline generates the current curated snapshot in `data/current_medal_snapshot.csv`. > Note: Milan Olympics medals are not final yet. Replace `source.medal_csv_url` with the official live Milan medal endpoint when available. ## Run ```bash python src/medals_agent.py --config config/pipeline_config.json --write-local ``` For backfill runs: ```bash python src/medals_agent.py --config config/pipeline_config.json --write-local --as-of-date 2026-02-10 ``` ## Databricks migration pattern Use this project as a bronze/silver ingestion step: 1. Schedule the script daily (Databricks Job, Airflow, or cron). 2. Persist CSV snapshots to cloud storage (DBFS/S3/ADLS). 3. In Databricks, load snapshot and `MERGE` into `main.olympics.milan_medal_snapshots` keyed by `(snapshot_date, country)`. 4. Keep running daily through Olympics end date. ## Agentic daily automation idea Implement a daily loop with 3 tools/roles: - **Data Fetch Agent**: reads official medal endpoint. - **Validation Agent**: checks schema + medal arithmetic + anomalies. - **Load Agent**: performs Databricks merge and posts run summary. This repository already implements fetch/validate/snapshot steps; wire the `DatabricksWriter.upsert` method in your Databricks runtime with `databricks-sql-connector` or Spark SQL merge. config/pipeline_config.jsonconfig/pipeline_config.json

New
+15-0
{ "source": { "medal_csv_url": "data/latest_medals_source.csv", "timeout_seconds": 30 }, "pipeline": { "olympics_name": "Milan Olympics", "output_csv_path": "data/current_medal_snapshot.csv" }, "databricks": { "catalog": "main", "schema": "olympics", "table": "milan_medal_snapshots" } } data/current_medal_snapshot.csvdata/current_medal_snapshot.csv
New
+11-0
country,gold,silver,bronze,total,snapshot_date,loaded_at_utc United States,40,44,42,126,2026-02-17,2026-02-17T18:35:24 China,40,27,24,91,2026-02-17,2026-02-17T18:35:24 Japan,20,12,13,45,2026-02-17,2026-02-17T18:35:24 Australia,18,19,16,53,2026-02-17,2026-02-17T18:35:24 France,16,26,22,64,2026-02-17,2026-02-17T18:35:24 Netherlands,15,7,12,34,2026-02-17,2026-02-17T18:35:24 Great Britain,14,22,29,65,2026-02-17,2026-02-17T18:35:24 South Korea,13,9,10,32,2026-02-17,2026-02-17T18:35:24 Italy,12,13,15,40,2026-02-17,2026-02-17T18:35:24 Germany,12,13,8,33,2026-02-17,2026-02-17T18:35:24 data/latest_medals_source.csvdata/latest_medals_source.csv
New
+11-0
country,gold,silver,bronze United States,40,44,42 China,40,27,24 Japan,20,12,13 Australia,18,19,16 France,16,26,22 Great Britain,14,22,29 Netherlands,15,7,12 South Korea,13,9,10 Italy,12,13,15 Germany,12,13,8 src/medals_agent.pysrc/medals_agent.py
New
+203-0
from __future__ import annotations import argparse import csv import datetime as dt import json import os from dataclasses import dataclass from pathlib import Path from typing import Dict, List from urllib.parse import urlparse from urllib.request import urlopen REQUIRED_COLUMNS = {"country", "gold", "silver", "bronze"} @dataclass class AgentRunContext: olympics_name: str as_of_date: dt.date source_url: str output_csv_path: str databricks_catalog: str databricks_schema: str databricks_table: str class MedalDataFetcher: def fetch(self, source_url: str, timeout_seconds: int = 30) -> List[Dict[str, str]]: parsed = urlparse(source_url) if parsed.scheme in {"http", "https"}: with urlopen(source_url, timeout=timeout_seconds) as response: payload = response.read().decode("utf-8") reader = csv.DictReader(payload.splitlines()) return list(reader) with open(source_url, "r", encoding="utf-8") as file: reader = csv.DictReader(file) return list(reader) class MedalDataTransformer: @staticmethod def _canonicalize_row(row: Dict[str, str]) -> Dict[str, str]: normalized = {k.strip().lower(): v for k, v in row.items()} if "noc" in normalized and "country" not in normalized: normalized["country"] = normalized["noc"] if "nation" in normalized and "country" not in normalized: normalized["country"] = normalized["nation"] return normalized def transform(self, raw_rows: List[Dict[str, str]], as_of_date: dt.date) -> List[Dict[str, str]]: transformed: List[Dict[str, str]] = [] for row in raw_rows: item = self._canonicalize_row(row) missing = REQUIRED_COLUMNS - set(item.keys()) if missing: raise ValueError(f"Missing required columns after canonicalization: {sorted(missing)}") gold = int(float(item["gold"] or 0)) silver = int(float(item["silver"] or 0)) bronze = int(float(item["bronze"] or 0)) total = gold + silver + bronze transformed.append( { "country": str(item["country"]).strip(), "gold": gold, "silver": silver, "bronze": bronze, "total": total, "snapshot_date": as_of_date.isoformat(), "loaded_at_utc": dt.datetime.utcnow().isoformat(timespec="seconds"), } ) transformed.sort(key=lambda x: (x["gold"], x["silver"], x["bronze"]), reverse=True) return transformed class MedalDataValidator: def validate(self, rows: List[Dict[str, str]]) -> List[str]: issues: List[str] = [] for idx, row in enumerate(rows): missing = REQUIRED_COLUMNS - set(row.keys()) if missing: issues.append(f"Row {idx}: Missing required columns: {sorted(missing)}") for col in ["gold", "silver", "bronze", "total"]: if int(row.get(col, 0)) < 0: issues.append(f"Row {idx}: Negative values detected in column '{col}'") expected_total = int(row.get("gold", 0)) + int(row.get("silver", 0)) + int(row.get("bronze", 0)) if int(row.get("total", 0)) != expected_total: issues.append(f"Row {idx}: Total mismatch") if not str(row.get("country", "")).strip(): issues.append(f"Row {idx}: Country is empty") return issues class DatabricksWriter: def upsert(self, rows: List[Dict[str, str]], catalog: str, schema: str, table: str) -> None: raise NotImplementedError( "Databricks SQL connector package is not installed in this environment. " "Run this pipeline in Databricks Job clusters with databricks-sql-connector and wire this method to SQL MERGE/INSERT." ) class MedalUpdateAgent: def __init__(self) -> None: self.fetcher = MedalDataFetcher() self.transformer = MedalDataTransformer() self.validator = MedalDataValidator() self.writer = DatabricksWriter() def run(self, context: AgentRunContext, timeout_seconds: int, write_local: bool, upsert_databricks: bool) -> Dict[str, str]: raw = self.fetcher.fetch(context.source_url, timeout_seconds=timeout_seconds) transformed = self.transformer.transform(raw, as_of_date=context.as_of_date) issues = self.validator.validate(transformed) if issues: raise ValueError("Validation failed: " + "; ".join(issues)) if write_local: output_path = Path(context.output_csv_path) output_path.parent.mkdir(parents=True, exist_ok=True) with open(output_path, "w", encoding="utf-8", newline="") as f: writer = csv.DictWriter( f, fieldnames=["country", "gold", "silver", "bronze", "total", "snapshot_date", "loaded_at_utc"], ) writer.writeheader() writer.writerows(transformed) if upsert_databricks: self.writer.upsert( transformed, catalog=context.databricks_catalog, schema=context.databricks_schema, table=context.databricks_table, ) return { "rows_processed": str(len(transformed)), "snapshot_date": context.as_of_date.isoformat(), "output_csv": context.output_csv_path, "target_table": f"{context.databricks_catalog}.{context.databricks_schema}.{context.databricks_table}", } def load_config(path: str) -> dict: with open(path, "r", encoding="utf-8") as f: return json.load(f) def build_context(cfg: dict, as_of_date: dt.date) -> AgentRunContext: return AgentRunContext( olympics_name=cfg["pipeline"]["olympics_name"], as_of_date=as_of_date, source_url=cfg["source"]["medal_csv_url"], output_csv_path=cfg["pipeline"]["output_csv_path"], databricks_catalog=cfg["databricks"]["catalog"], databricks_schema=cfg["databricks"]["schema"], databricks_table=cfg["databricks"]["table"], ) def parse_args() -> argparse.Namespace: parser = argparse.ArgumentParser(description="Agentic medal data updater for Milan Olympics") parser.add_argument("--config", default="config/pipeline_config.json") parser.add_argument("--as-of-date", help="Override snapshot date in YYYY-MM-DD") parser.add_argument("--write-local", action="store_true") parser.add_argument("--upsert-databricks", action="store_true") return parser.parse_args() def main() -> None: args = parse_args() cfg = load_config(args.config) as_of_date = dt.date.fromisoformat(args.as_of_date) if args.as_of_date else dt.date.today() context = build_context(cfg, as_of_date) agent = MedalUpdateAgent() summary = agent.run( context, timeout_seconds=int(cfg["source"].get("timeout_seconds", 30)), write_local=args.write_local, upsert_databricks=args.upsert_databricks, ) print("Medal update completed") for k, v in summary.items(): print(f"- {k}: {v}") if __name__ == "__main__": main() tests/test_validation.pytests/test_validation.py
New
+29-0
import datetime as dt import unittest from src.medals_agent import MedalDataTransformer, MedalDataValidator class ValidationTests(unittest.TestCase): def test_transform_adds_total_and_date(self): raw_rows = [ {"country": "A", "gold": "1", "silver": "2", "bronze": "3"}, {"country": "B", "gold": "0", "silver": "1", "bronze": "4"}, ] transformed = MedalDataTransformer().transform(raw_rows, dt.date(2026, 2, 7)) self.assertEqual(transformed[0]["total"], transformed[0]["gold"] + transformed[0]["silver"] + transformed[0]["bronze"]) self.assertEqual(transformed[0]["snapshot_date"], "2026-02-07") def test_validator_rejects_negative_values(self): rows = [ {"country": "A", "gold": -1, "silver": 0, "bronze": 1, "total": 0}, ] issues = MedalDataValidator().validate(rows) self.assertTrue(any("Negative values" in issue for issue in issues)) if __name__ == "__main__": unittest.main()

